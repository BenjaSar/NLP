{"cells":[{"cell_type":"markdown","metadata":{"id":"pfa39F4lsLf3"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Natural Language Processing\n","## LSTM Bot QA"]},{"cell_type":"markdown","metadata":{"id":"ZqO0PRcFsPTe"},"source":["### Datos\n","El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA).\\\n","[LINK](http://convai.io/data/)"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"bDFC0I3j9oFD"},"outputs":[],"source":["!pip install --upgrade --no-cache-dir gdown --quiet"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"cq3YXak9sGHd"},"outputs":[],"source":["import re\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchinfo in c:\\users\\fsiof\\anaconda3\\envs\\nlp_env\\lib\\site-packages (1.8.0)\n"]}],"source":["!pip3 install torchinfo\n","from torchinfo import summary"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["cuda = torch.cuda.is_available()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["import os\n","import platform\n","\n","if os.access('torch_helpers.py', os.F_OK) is False:\n","    if platform.system() == 'Windows':\n","        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n","    else:\n","        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["def sequence_acc(y_pred, y_test):\n","    y_pred_tag = y_pred.data.max(dim=-1,keepdim=True)[1]\n","    y_test_tag = y_test.data.max(dim=-1,keepdim=True)[1]\n","\n","    batch_size = y_pred_tag.shape[0]\n","    batch_acc = torch.zeros(batch_size)\n","    for b in range(batch_size):\n","        correct_results_sum = (y_pred_tag[b] == y_test_tag[b]).sum().float()\n","        batch_acc[b] = correct_results_sum / y_pred_tag[b].shape[0]\n","\n","    correct_results_sum = batch_acc.sum().float()\n","    acc = correct_results_sum / batch_size\n","    return acc\n","\n","def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n","    # Defino listas para realizar graficas de los resultados\n","    train_loss = []\n","    train_accuracy = []\n","    valid_loss = []\n","    valid_accuracy = []\n","\n","    # Defino mi loop de entrenamiento\n","\n","    for epoch in range(epochs):\n","\n","        epoch_train_loss = 0.0\n","        epoch_train_accuracy = 0.0\n","\n","        for train_encoder_input, train_decoder_input, train_target in train_loader:\n","            # Seteo los gradientes en cero ya que, por defecto, PyTorch\n","            # los va acumulando\n","            optimizer.zero_grad()\n","\n","            output = model(train_encoder_input.to(device), train_decoder_input.to(device))\n","\n","            # Computo el error de la salida comparando contra las etiquetas\n","            # por cada token en cada batch (sequence_loss)\n","            loss = 0\n","            for t in range(train_decoder_input.shape[1]):\n","                loss += criterion(output[:, t, :], train_target[:, t, :])\n","\n","            # Almaceno el error del batch para luego tener el error promedio de la epoca\n","            epoch_train_loss += loss.item()\n","\n","            # Computo el nuevo set de gradientes a lo largo de toda la red\n","            loss.backward()\n","\n","            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n","            optimizer.step()\n","\n","            # Calculo el accuracy del batch\n","            accuracy = sequence_acc(output, train_target)\n","            # Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca\n","            epoch_train_accuracy += accuracy.item()\n","\n","        # Calculo la media de error para la epoca de entrenamiento.\n","        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n","        epoch_train_loss = epoch_train_loss / len(train_loader)\n","        train_loss.append(epoch_train_loss)\n","        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)        \n","        train_accuracy.append(epoch_train_accuracy)\n","\n","        # Realizo el paso de validación computando error y accuracy, y\n","        # almacenando los valores para imprimirlos y graficarlos\n","        #valid_encoder_input, valid_decoder_input, valid_target = iter(valid_loader).next()\n","\n","        #valid_data, valid_target = iter(valid_loader).next()\n","        data_iter= iter(valid_loader)\n","        valid_encoder_input, valid_decoder_input, valid_target = next(data_iter)\n","        output = model(valid_encoder_input.to(device), valid_decoder_input.to(device))\n","        \n","        epoch_valid_loss = 0\n","        for t in range(train_decoder_input.shape[1]):\n","                epoch_valid_loss += criterion(output[:, t, :], valid_target[:, t, :])\n","        epoch_valid_loss = epoch_valid_loss.item()\n","\n","        valid_loss.append(epoch_valid_loss)\n","\n","        # Calculo el accuracy de la epoch\n","        epoch_valid_accuracy = sequence_acc(output, valid_target).item()\n","        valid_accuracy.append(epoch_valid_accuracy)\n","\n","        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\")\n","\n","    history = {\n","        \"loss\": train_loss,\n","        \"accuracy\": train_accuracy,\n","        \"val_loss\": valid_loss,\n","        \"val_accuracy\": valid_accuracy,\n","    }\n","    return history"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Datos"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"RHNkUaPp6aYq"},"outputs":[{"name":"stdout","output_type":"stream","text":["El dataset ya se encuentra descargado\n"]}],"source":["# Download the dataset\n","import os\n","import gdown\n","if os.access('data_volunteers.json', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n","    output = 'data_volunteers.json'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"WZy1-wgG-Rp7"},"outputs":[],"source":["# dataset_file\n","import json\n","\n","text_file = \"data_volunteers.json\"\n","with open(text_file) as f:\n","    data = json.load(f) # the data variable will be a dictionary\n"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"ue5qd54S-eew"},"outputs":[{"data":{"text/plain":["dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["# Observing the disponibles fields in every line of the dataset\n","data[0].keys()"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"jHBRAXPl-3dz"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of rows used: 6033\n"]}],"source":["chat_in = []\n","chat_out = []\n","\n","input_sentences = []\n","output_sentences = []\n","output_sentences_inputs = []\n","max_len = 30\n","\n","def clean_text(txt):\n","    txt = txt.lower()    \n","    txt.replace(\"\\'d\", \" had\")\n","    txt.replace(\"\\'s\", \" is\")\n","    txt.replace(\"\\'m\", \" am\")\n","    txt.replace(\"don't\", \"do not\")\n","    txt = re.sub(r'\\W+', ' ', txt)\n","    \n","    return txt\n","\n","for line in data:\n","    for i in range(len(line['dialog'])-1):\n","        # vamos separando el texto en \"preguntas\" (chat_in)\n","        # y \"respuestas\" (chat_out)\n","        chat_in = clean_text(line['dialog'][i]['text'])\n","        chat_out = clean_text(line['dialog'][i+1]['text'])\n","\n","        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n","            continue\n","\n","        input_sentence, output = chat_in, chat_out\n","        \n","        # output sentence (decoder_output) tiene <eos>\n","        output_sentence = output + ' <eos>'\n","        # output sentence input (decoder_input) tiene <sos>\n","        output_sentence_input = '<sos> ' + output\n","\n","        input_sentences.append(input_sentence)\n","        output_sentences.append(output_sentence)\n","        output_sentences_inputs.append(output_sentence_input)\n","\n","print(\"Number of rows used:\", len(input_sentences))"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"07L1qj8pC_l6"},"outputs":[{"data":{"text/plain":["('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["input_sentences[1], output_sentences[1], output_sentences_inputs[1]"]},{"cell_type":"markdown","metadata":{"id":"8P-ynUNP5xp6"},"source":["### 2 - Preprocessing\n","Realizar el preprocesamiento necesario para obtener:\n","- word2idx_inputs, max_input_len\n","- word2idx_outputs, max_out_len, num_words_output\n","- encoder_input_sequences, decoder_output_sequences, decoder_targets"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# Define the maximun number of words\n","MAX_VOCABULARY_SIZE = 8000"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["from torch_helpers import Tokenizer\n","\n","\n","# Create tokenizer for the input text and fit it to them\n","tokenizer_inputs= Tokenizer(num_words=MAX_VOCABULARY_SIZE)\n","tokenizer_inputs.fit_on_texts(input_sentences)\n","\n","# Tokenize and transform input texts to sequence of integers\n","input_integer_seq = tokenizer_inputs.texts_to_sequences(input_sentences)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Words in the vocabulary 1799\n","The longest sentence 9\n"]}],"source":["word2idx_inputs = tokenizer_inputs.word_index\n","print('Words in the vocabulary', len(word2idx_inputs))\n","\n","# Calculate the max length\n","max_input_len = max(len(sentence) for sentence in input_integer_seq )\n","print('The longest sentence', max_input_len)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[22]\n"]}],"source":["# Check the tokenization\n","print(input_integer_seq[200])"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["# Create tokenizer for the outpu text and fit it to them\n","#output_tokenizer = Tokenizer(num_words=MAX_VOCABULARY_SIZE, filters='')\n","output_tokenizer = Tokenizer(num_words=MAX_VOCABULARY_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n","\n","output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1806 unique output tokens.\n"]}],"source":["# Get the word to index mapping for output answer\n","word2idx_outputs = output_tokenizer.word_index\n","print('Found %s unique output tokens.' %len(word2idx_outputs))"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n","output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The longest sentence in the output 10\n"]}],"source":["# Calculate the max length for the ouput\n","max_output_len = max(len(sentence) for sentence in output_integer_seq)\n","print('The longest sentence in the output', max_output_len)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1807\n"]}],"source":["# One is added to include the toke of unknown word\n","number_word_output = min(len(word2idx_outputs) + 1, MAX_VOCABULARY_SIZE) \n","print(number_word_output)"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder input sequences shape:  (6033, 9)\n","Decoder input sequences shape:  (6033, 10)\n"]}],"source":["from torch_helpers import pad_sequences\n","\n","encoder_input_seq = pad_sequences(input_integer_seq, maxlen=max_input_len)\n","print('Encoder input sequences shape: ', encoder_input_seq.shape)\n","\n","decoder_input_seq = pad_sequences(output_integer_seq, maxlen=max_output_len, padding='post')\n","print('Decoder input sequences shape: ', decoder_input_seq.shape)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/plain":["10"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["max_output_len"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([6033, 10])"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["#from keras.utils.np_utils import to_categorical\n","decoder_output_seq = pad_sequences(output_integer_seq, maxlen=max_output_len, padding='post')\n","decoder_output_seq.shape\n","\n","torch.from_numpy(decoder_output_seq).shape"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"data":{"text/plain":["(6033, 10)"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["decoder_output_seq.shape"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["encoder_input_size: 9\n","decoder_input_size: 10\n","Output dim 1807\n"]}],"source":["class Data(Dataset):\n","    def __init__(self, encoder_input, decoder_input, decoder_output):\n","        # Convertir los arrays de numpy a tensores. \n","        # pytorch espera en general entradas 32bits\n","        self.encoder_inputs = torch.from_numpy(encoder_input.astype(np.int32))\n","        self.decoder_inputs = torch.from_numpy(decoder_input.astype(np.int32))\n","        # Transformar los datos a oneHotEncoding\n","        # la loss function esperan la salida float\n","        self.decoder_outputs = F.one_hot(torch.from_numpy(decoder_output).to(torch.int64), num_classes=number_word_output).float()\n","\n","        self.len = self.decoder_outputs.shape[0]\n","\n","    def __getitem__(self,index):\n","        return self.encoder_inputs[index], self.decoder_inputs[index], self.decoder_outputs[index]\n","\n","    def __len__(self):\n","        return self.len\n","\n","data_set = Data(encoder_input_seq, decoder_input_seq, decoder_output_seq)\n","\n","encoder_input_size = data_set.encoder_inputs.shape[1]\n","print(\"encoder_input_size:\", encoder_input_size)\n","\n","decoder_input_size = data_set.decoder_inputs.shape[1]\n","print(\"decoder_input_size:\", decoder_input_size)\n","\n","output_dim = data_set.decoder_outputs.shape[2]\n","print(\"Output dim\", output_dim)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tamaño del conjunto de entrenamiento: 4827\n","Tamaño del conjunto de validacion: 1206\n"]}],"source":["torch.manual_seed(42)\n","valid_set_size = int(data_set.len * 0.2)\n","train_set_size = data_set.len - valid_set_size\n","\n","train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n","valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n","\n","print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n","print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"_CJIsLBbj6rg"},"source":["### 3 - Preparing the embeddings\n","Using the embeddings of Glove or Fastext to transform the input tokens to vectors\n","\n","Based on subject Natural Language Processing - University of Buenos Aires - Embedded Systems Laboratory"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The embeddings gloveembedding.pkl have been downloaded yet\n"]}],"source":["#Download the embeddings\n","import os\n","import gdown\n","if os.access('gloveembedding.pkl', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download'\n","    output = 'gloveembedding.pkl'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"The embeddings gloveembedding.pkl have been downloaded yet\")"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","from pathlib import Path\n","from io import StringIO\n","import pickle\n","\n","class WordsEmbeddings(object):\n","    logger = logging.getLogger(__name__)\n","\n","    def __init__(self):\n","        # load the embeddings\n","        words_embedding_pkl = Path(self.PKL_PATH)\n","        if not words_embedding_pkl.is_file():\n","            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n","            assert words_embedding_txt.is_file(), 'Words embedding not available'\n","            embeddings = self.convert_model_to_pickle()\n","        else:\n","            embeddings = self.load_model_from_pickle()\n","        self.embeddings = embeddings\n","        # build the vocabulary hashmap\n","        index = np.arange(self.embeddings.shape[0])\n","        # Dictionary to translate of the embeeding to word idx \n","        self.word2idx = dict(zip(self.embeddings['word'], index))\n","        self.idx2word = dict(zip(index, self.embeddings['word']))\n","\n","    def get_words_embeddings(self, words):\n","        words_idxs = self.words2idxs(words)\n","        return self.embeddings[words_idxs]['embedding']\n","\n","    def words2idxs(self, words):\n","        return np.array([self.word2idx.get(word, -1) for word in words])\n","\n","    def idxs2words(self, idxs):\n","        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n","\n","    def load_model_from_pickle(self):\n","        self.logger.debug(\n","            'loading words embeddings from pickle {}'.format(\n","                self.PKL_PATH\n","            )\n","        )\n","        max_bytes = 2**28 - 1 # 256MB\n","        bytes_in = bytearray(0)\n","        input_size = os.path.getsize(self.PKL_PATH)\n","        with open(self.PKL_PATH, 'rb') as f_in:\n","            for _ in range(0, input_size, max_bytes):\n","                bytes_in += f_in.read(max_bytes)\n","        embeddings = pickle.loads(bytes_in)\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","    def convert_model_to_pickle(self):\n","        # create a numpy strctured array:\n","        # word     embedding\n","        # U50      np.float32[]\n","        # word_1   a, b, c\n","        # word_2   d, e, f\n","        # ...\n","        # word_n   g, h, i\n","        self.logger.debug(\n","            'converting and loading words embeddings from text file {}'.format(\n","                self.WORD_TO_VEC_MODEL_TXT_PATH\n","            )\n","        )\n","        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n","                     ('embedding', np.float32, (self.N_FEATURES,))]\n","        structure = np.dtype(structure)\n","        # load numpy array from disk using a generator\n","        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n","            embeddings_gen = (\n","                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n","                if len(line.split()[1:]) == self.N_FEATURES\n","            )\n","            embeddings = np.fromiter(embeddings_gen, structure)\n","        # add a null embedding\n","        null_embedding = np.array(\n","            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n","            dtype=structure\n","        )\n","        embeddings = np.concatenate([embeddings, null_embedding])\n","        # dump numpy array to disk using pickle\n","        max_bytes = 2**28 - 1 # # 256MB\n","        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n","        with open(self.PKL_PATH, 'wb') as f_out:\n","            for idx in range(0, len(bytes_out), max_bytes):\n","                f_out.write(bytes_out[idx:idx+max_bytes])\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","\n","class GloveEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n","    PKL_PATH = 'gloveembedding.pkl'\n","    N_FEATURES = 50\n","    #N_FEATURES = 51\n","    WORD_MAX_SIZE = 60\n","\n","class FasttextEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n","    PKL_PATH = 'fasttext.pkl'\n","    N_FEATURES = 300\n","    WORD_MAX_SIZE = 60"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["model_embeddings = GloveEmbeddings()"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["#model_embeddings = FasttextEmbeddings()"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["preparing embedding matrix...\n","number of null word embeddings: 38\n"]}],"source":["# Making the matrix embedding of the sequences\n","print('preparing embedding matrix...')\n","embed_dim = model_embeddings.N_FEATURES\n","words_not_found = []\n","\n","# The word index comes from tokenizer\n","\n","nb_words = min(MAX_VOCABULARY_SIZE, len(word2idx_inputs)) # vocab_size\n","embedding_matrix = np.zeros((nb_words, embed_dim))\n","for word, i in word2idx_inputs.items():\n","    if i >= nb_words:\n","        continue\n","    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","        \n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # words not found in embedding index will be all-zeros.\n","        words_not_found.append(word)\n","\n","print('number of null word embeddings:', np.sum(np.sum(embedding_matrix**2, axis=1) == 0))"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"data":{"text/plain":["(1799, 50)"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["# The embedding size\n","embedding_matrix.shape"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"data":{"text/plain":["1799"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["nb_words"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"data":{"text/plain":["(1799, 50)"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["embedding_matrix.shape"]},{"cell_type":"markdown","metadata":{"id":"3vKbhjtIwPgM"},"source":["### 4 - Training the model\n","Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"data":{"text/plain":["9"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["max_input_len"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Seq2Seq                                  [1, 10, 1807]             --\n","├─Encoder: 1-1                           [1, 1, 128]               --\n","│    └─Embedding: 2-1                    [1, 9, 50]                (89,950)\n","│    └─LSTM: 2-2                         [1, 9, 128]               92,160\n","├─Decoder: 1-2                           [1, 1807]                 --\n","│    └─Embedding: 2-3                    [1, 1, 50]                90,350\n","│    └─LSTM: 2-4                         [1, 1, 128]               92,160\n","│    └─Linear: 2-5                       [1, 1807]                 233,103\n","│    └─Softmax: 2-6                      [1, 1807]                 --\n","├─Decoder: 1-3                           [1, 1807]                 (recursive)\n","│    └─Embedding: 2-7                    [1, 1, 50]                (recursive)\n","│    └─LSTM: 2-8                         [1, 1, 128]               (recursive)\n","│    └─Linear: 2-9                       [1, 1807]                 (recursive)\n","│    └─Softmax: 2-10                     [1, 1807]                 --\n","├─Decoder: 1-4                           [1, 1807]                 (recursive)\n","│    └─Embedding: 2-11                   [1, 1, 50]                (recursive)\n","│    └─LSTM: 2-12                        [1, 1, 128]               (recursive)\n","│    └─Linear: 2-13                      [1, 1807]                 (recursive)\n","│    └─Softmax: 2-14                     [1, 1807]                 --\n","├─Decoder: 1-5                           [1, 1807]                 (recursive)\n","│    └─Embedding: 2-15                   [1, 1, 50]                (recursive)\n","│    └─LSTM: 2-16                        [1, 1, 128]               (recursive)\n","│    └─Linear: 2-17                      [1, 1807]                 (recursive)\n","│    └─Softmax: 2-18                     [1, 1807]                 --\n","├─Decoder: 1-6                           [1, 1807]                 (recursive)\n","│    └─Embedding: 2-19                   [1, 1, 50]                (recursive)\n","│    └─LSTM: 2-20                        [1, 1, 128]               (recursive)\n","│    └─Linear: 2-21                      [1, 1807]                 (recursive)\n","│    └─Softmax: 2-22                     [1, 1807]                 --\n","├─Decoder: 1-7                           [1, 1807]                 (recursive)\n","│    └─Embedding: 2-23                   [1, 1, 50]                (recursive)\n","│    └─LSTM: 2-24                        [1, 1, 128]               (recursive)\n","│    └─Linear: 2-25                      [1, 1807]                 (recursive)\n","│    └─Softmax: 2-26                     [1, 1807]                 --\n","├─Decoder: 1-8                           [1, 1807]                 (recursive)\n","│    └─Embedding: 2-27                   [1, 1, 50]                (recursive)\n","│    └─LSTM: 2-28                        [1, 1, 128]               (recursive)\n","│    └─Linear: 2-29                      [1, 1807]                 (recursive)\n","│    └─Softmax: 2-30                     [1, 1807]                 --\n","├─Decoder: 1-9                           [1, 1807]                 (recursive)\n","│    └─Embedding: 2-31                   [1, 1, 50]                (recursive)\n","│    └─LSTM: 2-32                        [1, 1, 128]               (recursive)\n","│    └─Linear: 2-33                      [1, 1807]                 (recursive)\n","│    └─Softmax: 2-34                     [1, 1807]                 --\n","├─Decoder: 1-10                          [1, 1807]                 (recursive)\n","│    └─Embedding: 2-35                   [1, 1, 50]                (recursive)\n","│    └─LSTM: 2-36                        [1, 1, 128]               (recursive)\n","│    └─Linear: 2-37                      [1, 1807]                 (recursive)\n","│    └─Softmax: 2-38                     [1, 1807]                 --\n","├─Decoder: 1-11                          [1, 1807]                 (recursive)\n","│    └─Embedding: 2-39                   [1, 1, 50]                (recursive)\n","│    └─LSTM: 2-40                        [1, 1, 128]               (recursive)\n","│    └─Linear: 2-41                      [1, 1807]                 (recursive)\n","│    └─Softmax: 2-42                     [1, 1807]                 --\n","==========================================================================================\n","Total params: 597,723\n","Trainable params: 507,773\n","Non-trainable params: 89,950\n","Total mult-adds (M): 5.08\n","==========================================================================================\n","Input size (MB): 0.46\n","Forward/backward pass size (MB): 0.17\n","Params size (MB): 2.39\n","Estimated Total Size (MB): 3.02\n","=========================================================================================="]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # num_embeddings = vocab_size, definido por le Tokenizador\n","        # embedding_dim = 50 --> dimensión de los embeddings utilizados\n","        self.lstm_size = 128\n","        self.num_layers = 1\n","        self.embedding_dim = embed_dim\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n","        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n","        self.embedding.weight.requires_grad = False  # marcar como layer no entrenable (freeze)\n","        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n","                            num_layers=self.num_layers) # LSTM layer\n","\n","    def forward(self, x):\n","        out = self.embedding(x)\n","        lstm_output, (ht, ct) = self.lstm(out)\n","        return (ht, ct)\n","\n","class Decoder(nn.Module):\n","    def __init__(self, vocab_size, output_dim):\n","        super().__init__()\n","        # num_embeddings = vocab_size, definido por le Tokenizador\n","        # embedding_dim = 50 --> dimensión de los embeddings utilizados\n","        self.lstm_size = 128\n","        self.num_layers = 1\n","        self.embedding_dim = embed_dim\n","        self.output_dim = output_dim\n","\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n","                            num_layers=self.num_layers) # LSTM layer\n","        self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=self.output_dim) # Fully connected layer\n","\n","        self.softmax = nn.Softmax(dim=1) # normalize in dim 1\n","\n","    def forward(self, x, prev_state):\n","        out = self.embedding(x)\n","        lstm_output, (ht, ct) = self.lstm(out, prev_state)\n","        out = self.softmax(self.fc1(lstm_output[:,-1,:])) # take last output (last seq)\n","        return out, (ht, ct)\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        \n","        assert encoder.lstm_size == decoder.lstm_size, \\\n","            \"Hidden dimensions of encoder and decoder must be equal!\"\n","        assert encoder.num_layers == decoder.num_layers, \\\n","            \"Encoder and decoder must have equal number of layers!\"\n","        \n","    def forward(self, encoder_input, decoder_input):\n","        batch_size = decoder_input.shape[0]\n","        decoder_input_len = decoder_input.shape[1]\n","        vocab_size = self.decoder.output_dim\n","        \n","        # tensor para almacenar la salida\n","        # (batch_size, sentence_len, one_hot_size)\n","        outputs = torch.zeros(batch_size, decoder_input_len, vocab_size)\n","        \n","        # ultimo hidden state del encoder, primer estado oculto del decoder\n","        prev_state = self.encoder(encoder_input)\n","      \n","        # En la primera iteracion se toma el primer token de target (<sos>)\n","        input = decoder_input[:, 0:1]\n","\n","        for t in range(decoder_input_len):\n","            # t --> token index\n","\n","            # utilizamos método \"teacher forcing\", es decir que durante\n","            # el entrenamiento no realimentamos la salida del decoder\n","            # sino el token correcto que sigue en target\n","            input = decoder_input[:, t:t+1]\n","\n","            # ingresar cada token embedding, uno por uno junto al hidden state\n","            # recibir el output del decoder (softmax)\n","            output, prev_state = self.decoder(input, prev_state)\n","            top1 = output.argmax(1).view(-1, 1)\n","\n","            # Sino se usará \"teacher forcing\" habría que descomentar\n","            # esta linea.\n","            # Hay ejemplos dandos vuelta en donde se utilza un random \n","            # para ver en cada vuelta que técnica se aplica\n","            #input = top1            \n","\n","            # guardar cada salida (softmax)\n","            outputs[:, t, :] = output\n","\n","        return outputs\n","\n","encoder = Encoder(vocab_size=nb_words)\n","if cuda: encoder.cuda()\n","# decoder --> vocab_size == output_dim --> porque recibe y devuelve palabras en el mismo vocabulario\n","decoder = Decoder(vocab_size=number_word_output, output_dim=number_word_output)\n","if cuda: decoder.cuda()\n","\n","model = Seq2Seq(encoder, decoder)\n","if cuda: model.cuda()\n","\n","# Crear el optimizador la una función de error\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n","criterion = torch.nn.CrossEntropyLoss()  # Para clasificación multi categórica\n","\n","summary(model, input_data=(data_set[0:1][0], data_set[0:1][1]))"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/10 - Train loss 67.211 - Train accuracy 0.782 - Valid Loss 67.035 - Valid accuracy 0.797\n","Epoch: 2/10 - Train loss 66.915 - Train accuracy 0.809 - Valid Loss 66.880 - Valid accuracy 0.812\n","Epoch: 3/10 - Train loss 66.867 - Train accuracy 0.814 - Valid Loss 66.789 - Valid accuracy 0.822\n","Epoch: 4/10 - Train loss 66.821 - Train accuracy 0.818 - Valid Loss 66.757 - Valid accuracy 0.825\n","Epoch: 5/10 - Train loss 66.790 - Train accuracy 0.821 - Valid Loss 66.724 - Valid accuracy 0.828\n","Epoch: 6/10 - Train loss 66.760 - Train accuracy 0.824 - Valid Loss 66.739 - Valid accuracy 0.825\n","Epoch: 7/10 - Train loss 66.741 - Train accuracy 0.826 - Valid Loss 66.723 - Valid accuracy 0.828\n","Epoch: 8/10 - Train loss 66.750 - Train accuracy 0.825 - Valid Loss 66.723 - Valid accuracy 0.828\n","Epoch: 9/10 - Train loss 66.745 - Train accuracy 0.826 - Valid Loss 66.722 - Valid accuracy 0.828\n","Epoch: 10/10 - Train loss 66.743 - Train accuracy 0.826 - Valid Loss 66.722 - Valid accuracy 0.828\n"]}],"source":["history1 = train(model,\n","                train_loader,\n","                valid_loader,\n","                optimizer,\n","                criterion,\n","                epochs=20\n","                )"]},{"cell_type":"markdown","metadata":{"id":"Zbwn0ekDy_s2"},"source":["### 5 - Inference\n","Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
