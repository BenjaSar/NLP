{"cells":[{"cell_type":"markdown","metadata":{"id":"pfa39F4lsLf3"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Natural Language Processing\n","## LSTM Bot QA"]},{"cell_type":"markdown","metadata":{"id":"ZqO0PRcFsPTe"},"source":["### Datos\n","El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA).\\\n","[LINK](http://convai.io/data/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDFC0I3j9oFD"},"outputs":[],"source":["!pip install --upgrade --no-cache-dir gdown --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cq3YXak9sGHd"},"outputs":[],"source":["import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from keras.preprocessing.text import one_hot\n","from tensorflow.keras.utils import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Activation, Dropout, Dense\n","#from keras.layers.core import Activation, Dropout, Dense\n","from keras.layers import Flatten, LSTM, SimpleRNN\n","from keras.models import Model\n","from tensorflow.keras.layers import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHNkUaPp6aYq"},"outputs":[],"source":["# Descargar la carpeta de dataset\n","import os\n","import gdown\n","if os.access('data_volunteers.json', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n","    output = 'data_volunteers.json'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZy1-wgG-Rp7"},"outputs":[],"source":["# dataset_file\n","import json\n","\n","text_file = \"data_volunteers.json\"\n","with open(text_file) as f:\n","    data = json.load(f) # the data variable will be a dictionary\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ue5qd54S-eew"},"outputs":[],"source":["# Observing the disponibles fields in every line of the dataset\n","data[0].keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHBRAXPl-3dz"},"outputs":[],"source":["chat_in = []\n","chat_out = []\n","\n","input_sentences = []\n","output_sentences = []\n","output_sentences_inputs = []\n","max_len = 30\n","\n","def clean_text(txt):\n","    txt = txt.lower()    \n","    txt.replace(\"\\'d\", \" had\")\n","    txt.replace(\"\\'s\", \" is\")\n","    txt.replace(\"\\'m\", \" am\")\n","    txt.replace(\"don't\", \"do not\")\n","    txt = re.sub(r'\\W+', ' ', txt)\n","    \n","    return txt\n","\n","for line in data:\n","    for i in range(len(line['dialog'])-1):\n","        # vamos separando el texto en \"preguntas\" (chat_in)\n","        # y \"respuestas\" (chat_out)\n","        chat_in = clean_text(line['dialog'][i]['text'])\n","        chat_out = clean_text(line['dialog'][i+1]['text'])\n","\n","        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n","            continue\n","\n","        input_sentence, output = chat_in, chat_out\n","        \n","        # output sentence (decoder_output) tiene <eos>\n","        output_sentence = output + ' <eos>'\n","        # output sentence input (decoder_input) tiene <sos>\n","        output_sentence_input = '<sos> ' + output\n","\n","        input_sentences.append(input_sentence)\n","        output_sentences.append(output_sentence)\n","        output_sentences_inputs.append(output_sentence_input)\n","\n","print(\"Number of rows used:\", len(input_sentences))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07L1qj8pC_l6"},"outputs":[],"source":["input_sentences[1], output_sentences[1], output_sentences_inputs[1]"]},{"cell_type":"markdown","metadata":{"id":"8P-ynUNP5xp6"},"source":["### 2 - Preprocessing\n","Realizar el preprocesamiento necesario para obtener:\n","- word2idx_inputs, max_input_len\n","- word2idx_outputs, max_out_len, num_words_output\n","- encoder_input_sequences, decoder_output_sequences, decoder_targets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the maximun number of words\n","MAX_VOCABULARY_SIZE = 8000"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","\n","\n","# Create tokenizer for the input text and fit it to them\n","tokenizer_inputs= Tokenizer(num_words=MAX_VOCABULARY_SIZE)\n","tokenizer_inputs.fit_on_texts(input_sentences)\n","\n","# Tokenize and transform input texts to sequence of integers\n","input_integer_seq = tokenizer_inputs.texts_to_sequences(input_sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word2idx_inputs = tokenizer_inputs.word_index\n","print('Words in the vocabulary', len(word2idx_inputs))\n","\n","# Calculate the max length\n","max_input_len = max(len(sentence) for sentence in input_integer_seq )\n","print('The longest sentence', max_input_len)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check the tokenization\n","print(input_integer_seq[200])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create tokenizer for the outpu text and fit it to them\n","#output_tokenizer = Tokenizer(num_words=MAX_VOCABULARY_SIZE, filters='')\n","output_tokenizer = Tokenizer(num_words=MAX_VOCABULARY_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n","\n","output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the word to index mapping for output answer\n","word2idx_outputs = output_tokenizer.word_index\n","print('Found %s unique output tokens.' %len(word2idx_outputs))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n","output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the max length for the ouput\n","max_output_len = max(len(sentence) for sentence in output_integer_seq)\n","print('The longest sentence in the output', max_input_len)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# One is added to include the toke of unknown word\n","number_word_output = min(len(word2idx_outputs) + 1, MAX_VOCABULARY_SIZE) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoder_input_seq = pad_sequences(input_integer_seq, maxlen=max_input_len)\n","print('Encoder input sequences shape: ', encoder_input_seq.shape)\n","\n","decoder_input_seq = pad_sequences(output_integer_seq, maxlen=max_output_len, padding='post')\n","print('Decoder input sequences shape: ', decoder_input_seq.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#from keras.utils.np_utils import to_categorical\n","from keras.utils import to_categorical\n","decoder_output_seq = pad_sequences(output_integer_seq, maxlen=max_output_len, padding='post')\n","decoder_output_seq.shape\n","\n","decoder_target = to_categorical(decoder_output_seq, num_classes=number_word_output)\n","decoder_target.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["decoder_output_seq.shape"]},{"cell_type":"markdown","metadata":{"id":"_CJIsLBbj6rg"},"source":["### 3 - Preparing the embeddings\n","Using the embeddings of Glove or Fastext to transform the input tokens to vectors\n","\n","Based on subject Natural Language Processing - University of Buenos Aires - Embedded Systems Laboratory"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Download the embeddings\n","import os\n","import gdown\n","if os.access('gloveembedding.pkl', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download'\n","    output = 'gloveembedding.pkl'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"The embeddings gloveembedding.pkl have been downloaded yet\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","from pathlib import Path\n","from io import StringIO\n","import pickle\n","\n","class WordsEmbeddings(object):\n","    logger = logging.getLogger(__name__)\n","\n","    def __init__(self):\n","        # load the embeddings\n","        words_embedding_pkl = Path(self.PKL_PATH)\n","        if not words_embedding_pkl.is_file():\n","            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n","            assert words_embedding_txt.is_file(), 'Words embedding not available'\n","            embeddings = self.convert_model_to_pickle()\n","        else:\n","            embeddings = self.load_model_from_pickle()\n","        self.embeddings = embeddings\n","        # build the vocabulary hashmap\n","        index = np.arange(self.embeddings.shape[0])\n","        # Dictionary to translate of the embeeding to word idx \n","        self.word2idx = dict(zip(self.embeddings['word'], index))\n","        self.idx2word = dict(zip(index, self.embeddings['word']))\n","\n","    def get_words_embeddings(self, words):\n","        words_idxs = self.words2idxs(words)\n","        return self.embeddings[words_idxs]['embedding']\n","\n","    def words2idxs(self, words):\n","        return np.array([self.word2idx.get(word, -1) for word in words])\n","\n","    def idxs2words(self, idxs):\n","        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n","\n","    def load_model_from_pickle(self):\n","        self.logger.debug(\n","            'loading words embeddings from pickle {}'.format(\n","                self.PKL_PATH\n","            )\n","        )\n","        max_bytes = 2**28 - 1 # 256MB\n","        bytes_in = bytearray(0)\n","        input_size = os.path.getsize(self.PKL_PATH)\n","        with open(self.PKL_PATH, 'rb') as f_in:\n","            for _ in range(0, input_size, max_bytes):\n","                bytes_in += f_in.read(max_bytes)\n","        embeddings = pickle.loads(bytes_in)\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","    def convert_model_to_pickle(self):\n","        # create a numpy strctured array:\n","        # word     embedding\n","        # U50      np.float32[]\n","        # word_1   a, b, c\n","        # word_2   d, e, f\n","        # ...\n","        # word_n   g, h, i\n","        self.logger.debug(\n","            'converting and loading words embeddings from text file {}'.format(\n","                self.WORD_TO_VEC_MODEL_TXT_PATH\n","            )\n","        )\n","        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n","                     ('embedding', np.float32, (self.N_FEATURES,))]\n","        structure = np.dtype(structure)\n","        # load numpy array from disk using a generator\n","        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n","            embeddings_gen = (\n","                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n","                if len(line.split()[1:]) == self.N_FEATURES\n","            )\n","            embeddings = np.fromiter(embeddings_gen, structure)\n","        # add a null embedding\n","        null_embedding = np.array(\n","            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n","            dtype=structure\n","        )\n","        embeddings = np.concatenate([embeddings, null_embedding])\n","        # dump numpy array to disk using pickle\n","        max_bytes = 2**28 - 1 # # 256MB\n","        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n","        with open(self.PKL_PATH, 'wb') as f_out:\n","            for idx in range(0, len(bytes_out), max_bytes):\n","                f_out.write(bytes_out[idx:idx+max_bytes])\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","\n","class GloveEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n","    PKL_PATH = 'gloveembedding.pkl'\n","    N_FEATURES = 50\n","    #N_FEATURES = 51\n","    WORD_MAX_SIZE = 60\n","\n","class FasttextEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n","    PKL_PATH = 'fasttext.pkl'\n","    N_FEATURES = 300\n","    WORD_MAX_SIZE = 60"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_embeddings = GloveEmbeddings()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Making the matrix embedding of the sequences\n","print('preparing embedding matrix...')\n","embed_dim = model_embeddings.N_FEATURES\n","words_not_found = []\n","\n","# The word index comes from tokenizer\n","\n","nb_words = min(MAX_VOCABULARY_SIZE, len(word2idx_inputs)) # vocab_size\n","embedding_matrix = np.zeros((nb_words, embed_dim))\n","for word, i in word2idx_inputs.items():\n","    if i >= nb_words:\n","        continue\n","    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","        \n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # words not found in embedding index will be all-zeros.\n","        words_not_found.append(word)\n","\n","print('number of null word embeddings:', np.sum(np.sum(embedding_matrix**2, axis=1) == 0))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The embedding size\n","embedding_matrix.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(embedding_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embedding_matrix_c = np.c_[embedding_matrix, np.zeros(embedding_matrix.shape[0])]\n","embedding_matrix[10]"]},{"cell_type":"markdown","metadata":{"id":"3vKbhjtIwPgM"},"source":["### 4 - Training the model\n","Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_input_len"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_units = 128\n","\n","# define training encoder\n","encoder_inputs = Input(shape=(max_input_len))\n","\n","encoder_embedding_layer = Embedding(\n","          input_dim=nb_words,  # definido en el Tokenizador\n","          output_dim=embed_dim,  # dimensión de los embeddings utilizados\n","          input_length=max_input_len, # tamaño máximo de la secuencia de entrada\n","          weights=[embedding_matrix],  # matrix de embeddings\n","          trainable=False)      # marcar como layer no entrenable\n","\n","encoder_inputs_x = encoder_embedding_layer(encoder_inputs)\n","\n","encoder = LSTM(n_units, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs_x)\n","encoder_states = [state_h, state_c]\n","\n","# define training decoder\n","decoder_inputs = Input(shape=(max_output_len))\n","decoder_embedding_layer = Embedding(input_dim=number_word_output, output_dim=n_units, input_length=max_output_len)\n","decoder_inputs_x = decoder_embedding_layer(decoder_inputs)\n","\n","decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n","\n","# Dense\n","decoder_dense = Dense(number_word_output, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.utils import plot_model\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model only encoder\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","from keras.utils import plot_model\n","plot_model(encoder_model, to_file='encoder_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model only decoder (for making inferences)\n","decoder_state_input_h = Input(shape=(n_units,))\n","decoder_state_input_c = Input(shape=(n_units,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","decoder_inputs_single = Input(shape=(1,))\n","decoder_inputs_single_x = decoder_embedding_layer(decoder_inputs_single)\n","\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model([decoder_inputs_single] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n","\n","plot_model(decoder_model, to_file='decoder_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Based on https://stackoverflow.com/questions/65514944/tensorflow-embeddings-invalidargumenterror-indices18-16-11905-is-not-in-0\n","\n","hist = model.fit(\n","    [encoder_input_seq, decoder_input_seq],\n","    decoder_target,\n","    epochs=30, \n","    validation_split=0.2)"]},{"cell_type":"markdown","metadata":{"id":"Zbwn0ekDy_s2"},"source":["### 5 - Inference\n","Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
