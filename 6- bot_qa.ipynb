{"cells":[{"cell_type":"markdown","metadata":{"id":"pfa39F4lsLf3"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Natural Language Processing\n","## LSTM Bot QA"]},{"cell_type":"markdown","metadata":{"id":"ZqO0PRcFsPTe"},"source":["### Datos\n","El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA).\\\n","[LINK](http://convai.io/data/)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"bDFC0I3j9oFD"},"outputs":[],"source":["!pip install --upgrade --no-cache-dir gdown --quiet"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"cq3YXak9sGHd"},"outputs":[],"source":["import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from keras.preprocessing.text import one_hot\n","from tensorflow.keras.utils import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Activation, Dropout, Dense\n","#from keras.layers.core import Activation, Dropout, Dense\n","from keras.layers import Flatten, LSTM, SimpleRNN\n","from keras.models import Model\n","from tensorflow.keras.layers import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"RHNkUaPp6aYq"},"outputs":[{"name":"stdout","output_type":"stream","text":["El dataset ya se encuentra descargado\n"]}],"source":["# Descargar la carpeta de dataset\n","import os\n","import gdown\n","if os.access('data_volunteers.json', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n","    output = 'data_volunteers.json'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"WZy1-wgG-Rp7"},"outputs":[],"source":["# dataset_file\n","import json\n","\n","text_file = \"data_volunteers.json\"\n","with open(text_file) as f:\n","    data = json.load(f) # the data variable will be a dictionary\n"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"ue5qd54S-eew"},"outputs":[{"data":{"text/plain":["dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["# Observing the disponibles fields in every line of the dataset\n","data[0].keys()"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"jHBRAXPl-3dz"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of rows used: 6033\n"]}],"source":["chat_in = []\n","chat_out = []\n","\n","input_sentences = []\n","output_sentences = []\n","output_sentences_inputs = []\n","max_len = 30\n","\n","def clean_text(txt):\n","    txt = txt.lower()    \n","    txt.replace(\"\\'d\", \" had\")\n","    txt.replace(\"\\'s\", \" is\")\n","    txt.replace(\"\\'m\", \" am\")\n","    txt.replace(\"don't\", \"do not\")\n","    txt = re.sub(r'\\W+', ' ', txt)\n","    \n","    return txt\n","\n","for line in data:\n","    for i in range(len(line['dialog'])-1):\n","        # vamos separando el texto en \"preguntas\" (chat_in)\n","        # y \"respuestas\" (chat_out)\n","        chat_in = clean_text(line['dialog'][i]['text'])\n","        chat_out = clean_text(line['dialog'][i+1]['text'])\n","\n","        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n","            continue\n","\n","        input_sentence, output = chat_in, chat_out\n","        \n","        # output sentence (decoder_output) tiene <eos>\n","        output_sentence = output + ' <eos>'\n","        # output sentence input (decoder_input) tiene <sos>\n","        output_sentence_input = '<sos> ' + output\n","\n","        input_sentences.append(input_sentence)\n","        output_sentences.append(output_sentence)\n","        output_sentences_inputs.append(output_sentence_input)\n","\n","print(\"Number of rows used:\", len(input_sentences))"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"07L1qj8pC_l6"},"outputs":[{"data":{"text/plain":["('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["input_sentences[1], output_sentences[1], output_sentences_inputs[1]"]},{"cell_type":"markdown","metadata":{"id":"8P-ynUNP5xp6"},"source":["### 2 - Preprocessing\n","Realizar el preprocesamiento necesario para obtener:\n","- word2idx_inputs, max_input_len\n","- word2idx_outputs, max_out_len, num_words_output\n","- encoder_input_sequences, decoder_output_sequences, decoder_targets"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# Define the maximun number of words\n","MAX_VOCABULARY_SIZE = 8000"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","\n","\n","# Create tokenizer for the input text and fit it to them\n","tokenizer_inputs= Tokenizer(num_words=MAX_VOCABULARY_SIZE)\n","tokenizer_inputs.fit_on_texts(input_sentences)\n","\n","# Tokenize and transform input texts to sequence of integers\n","input_integer_seq = tokenizer_inputs.texts_to_sequences(input_sentences)"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Words in the vocabulary 1799\n","The longest sentence 9\n"]}],"source":["word2indx_inputs = tokenizer_inputs.word_index\n","print('Words in the vocabulary', len(word2indx_inputs))\n","\n","# Calculate the max length\n","max_input_len = max(len(sentence) for sentence in input_integer_seq )\n","print('The longest sentence', max_input_len)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[22]\n"]}],"source":["# Check the tokenization\n","print(input_integer_seq[200])"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# Create tokenizer for the outpu text and fit it to them\n","output_tokenizer = Tokenizer(num_words=MAX_VOCABULARY_SIZE, filters='')\n","#output_tokenizer = Tokenizer(num_words=MAX_VOCABULARY_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n","\n","output_tokenizer.fit_on_texts(output_sentence)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5 unique output tokens.\n"]}],"source":["# Get the word to index mapping for output answer\n","word2indx_outputs = output_tokenizer.word_index\n","print('Found %s unique output tokens.' %len(word2indx_outputs))"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["output_integer_seq = output_tokenizer.texts_to_sequences(output_sentence)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The longest sentence in the output 9\n"]}],"source":["# Calculate the max length for the ouput\n","max_output_len = max(len(output_sentence) for output_sentence in output_integer_seq)\n","print('The longest sentence in the output', max_input_len)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["# One is added to include the toke of unknown word\n","number_word_output = min(len(word2indx_outputs) + 1, MAX_VOCABULARY_SIZE) "]},{"cell_type":"markdown","metadata":{"id":"_CJIsLBbj6rg"},"source":["### 3 - Preparing the embeddings\n","Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores"]},{"cell_type":"markdown","metadata":{"id":"3vKbhjtIwPgM"},"source":["### 4 - Training the model\n","Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."]},{"cell_type":"markdown","metadata":{"id":"Zbwn0ekDy_s2"},"source":["### 5 - Inference\n","Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
