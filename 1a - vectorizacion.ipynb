{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Natual Language Processing\n","## Vectorization\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"PUbfVnzIIoMj"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"RIO7b8GjAC17"},"outputs":[],"source":["corpus = np.array(\n","    ['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias'])"]},{"cell_type":"markdown","metadata":{"id":"8WqdaTmO8P1r"},"source":["Documento 1 --> que dia es hoy \\\n","Documento 2 --> martes el dia de hoy es martes \\\n","Documento 3 --> martes muchas gracias"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n","- Cada documento transformarlo en una lista de términos\n","- Armar un vector de términos no repetidos de todos los documentos"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"3ZqTOZzDI7uv"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\fSIoF\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\fSIoF\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"]},{"name":"stdout","output_type":"stream","text":["['que', 'dia', 'es', 'hoy']\n","['martes', 'el', 'dia', 'de', 'hoy', 'es', 'martes']\n","['martes', 'muchas', 'gracias']\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","\n","\n","# List of words\n","document1 = \"que dia es hoy\"\n","document2 = \"martes el dia de hoy es martes\"\n","document3 = \"martes muchas gracias\"\n","\n","\n","def document2list(lst):\n","    return ([i for i in lst.split()])\n","\n","\n","print(document2list(document1))\n","\n","print(document2list(document2))\n","\n","document3_to_list = nltk.word_tokenize(document3)\n","print(document3_to_list)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['que', 'dia', 'es', 'hoy']\n","['martes', 'el', 'dia', 'de', 'hoy', 'es', 'martes']\n","['martes', 'muchas', 'gracias']\n"]}],"source":["# Transform text to list using python\n","document1_to_list = document1.split()\n","print(document1_to_list)\n","document2_to_list = document2.split()\n","print(document2_to_list)\n","document3_to_list = document3.split()\n","print(document3_to_list)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'que': 8, 'dia': 1, 'es': 3, 'hoy': 5, 'martes': 6, 'el': 2, 'de': 0, 'muchas': 7, 'gracias': 4}\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","corpus = [\"que dia es hoy\",\n","          \"martes el dia de hoy es martes\",\n","          \"martes muchas gracias\"]\n","\n","# Create an object of CountVectorizer class\n","vectorizer = CountVectorizer()\n","\n","# Tokenize and buil vocabulary\n","vectorizer.fit(corpus)\n","\n","# Encode\n","vector = vectorizer.transform(corpus)\n","print(vectorizer.vocabulary_)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import re\n","import string\n","\n","\n","def count_occurrence(str):\n","    pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]'\n","    str_modified = re.sub(pattern, '', str)\n","    string_modified = ''.join([c for c in str_modified if c not in string.punctuation])\n","    text = string_modified.split()\n","    counts = {}    \n","    for word in text:\n","        if counts.get(word) is None:\n","            counts[word] = 1\n","        else:\n","            counts[word]+= 1\n","\n","    duplicated = {key:value for key, value in counts.items() if value==1}\n","\n","    sorted_counts = dict(\n","        sorted(duplicated.items(), key=lambda item: item[1], reverse=True)\n","    )\n","\n","    return sorted_counts\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["{'que': 1, 'el': 1, 'de': 1, 'muchas': 1, 'gracias': 1}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["corpus = \"que dia es hoy, martes el dia de hoy es martes, martes muchas gracias\"\n","count_occurrence(corpus)"]},{"cell_type":"markdown","metadata":{"id":"RUhH983FI7It"},"source":["### 2- OneHot encoding\n","Dada una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"Os0AAQo6I6Z1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0]\n"," [0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0]\n"," [1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1]]\n"]}],"source":["# Function with one-hot-encoding\n","from sklearn.preprocessing import Binarizer\n","\n","corpus = ['The cat sat on the mat.',\n","          'The dog chased the cat.',\n","           'CS224n at Stanford is the best NLP class you can ever take!']\n","\n","\n","freq = CountVectorizer()\n","corpus = freq.fit_transform(corpus)\n","one_hot_enconding = Binarizer()\n","matrix_words = one_hot_enconding.fit_transform(corpus.toarray())\n","print(matrix_words)"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["def one_hot_encoding(corpus):\n","    unique_words = set()\n","    # Convert the corpus in arrays of documents\n","    for document in corpus:\n","        for word in document.split():\n","            unique_words.add(word.lower())\n","\n","    # Word to index \n","    word_to_index = {}\n","    for i, word in  enumerate(unique_words):\n","        word_to_index[word] = i\n","    ohe_vector = []\n","    document_vectors = []\n","\n","    for document in corpus:\n","        for word in document.split():\n","            vector = np.zeros(len(unique_words))\n","            vector[word_to_index[word.lower()]] = 1\n","            document_vectors.append(vector)\n","        ohe_vector.append(document_vectors)\n","    \n","    return ohe_vector"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"data":{"text/plain":["[[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         1., 0.]),\n","  array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 1.]),\n","  array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.])],\n"," [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         1., 0.]),\n","  array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 1.]),\n","  array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.])],\n"," [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         1., 0.]),\n","  array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 1.]),\n","  array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0.]),\n","  array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n","         0., 0.]),\n","  array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0.])]]"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["corpus = ['The cat sat on the mat.',\n","          'The dog chased the cat.',\n","           'CS224n at Stanford is the best NLP class you can ever take!']\n","one_hot_encoding(corpus)"]},{"cell_type":"markdown","metadata":{"id":"IIyWGmCpJVQL"},"source":["### 3- Vectores de frecuencia\n","Dada una lista de textos, devolver una matriz con la representación de frecuencia de estos"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"yqij_7eHJbUi"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'The': 3, 'cat': 7, 'sat': 15, 'on': 14, 'the': 17, 'mat': 13, 'dog': 10, 'chased': 8, 'CS224n': 0, 'at': 4, 'Stanford': 2, 'is': 12, 'best': 5, 'NLP': 1, 'class': 9, 'you': 18, 'can': 6, 'ever': 11, 'take': 16}\n","[[0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0]\n"," [0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0]\n"," [1 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1]]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","sentences = ['The cat sat on the mat.',\n","             'The dog chased the cat.',\n","             'CS224n at Stanford is the best NLP class you can ever take!']\n","\n","vectorizer = CountVectorizer(lowercase=False)\n","\n","vectorizer.fit(sentences)\n","\n","print(vectorizer.vocabulary_)\n","\n","print(vectorizer.transform(sentences).toarray())"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["list_documents=[]\n","list_text=[]\n","word_list=[]\n","word_dictionary = []\n","\n","def frequency(corpus):\n","    for document in corpus:\n","        list_documents.append(document)\n","        pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]'\n","        for str in list_documents:\n","            str_modified = re.sub(pattern, '', str)\n","            string_modified = ''.join([c for c in str_modified if c not in string.punctuation])\n","            text = string_modified.split()\n","            for word in text:\n","                word_list.append(word)\n","        list_text.append(string_modified.lower().split())\n","        for i in word_list:\n","            if i not in word_dictionary:\n","                word_dictionary.append(i.lower())\n","        matrix_fq = np.zeros(len(word_dictionary))\n","        for words in word_dictionary:\n","            for j in range(len(list_documents)):\n","                if words in list_documents[j] and words in word_dictionary:\n","                    index = word_dictionary.index(words)\n","                    matrix_fq[index] += 1\n","        \n","    \n","    return  matrix_fq"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"data":{"text/plain":["array([27.,  2.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,\n","        0.,  0.,  0.,  3.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.])"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["sentences = ['The cat sat on the mat.',\n","             'The dog chased the cat.',\n","             'CS224n at Stanford is the best NLP class you can ever take!']\n","\n","frequency(sentences)"]},{"cell_type":"markdown","metadata":{"id":"z_Ot8HvWJcBu"},"source":["### 4- TF-IDF\n","Data una lista de textos, devolver una matriz con la representacion TFIDF"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"waG_oWtpJjRw"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.1796053 0.1796053 0.1796053 0.1796053 0.1796053 0.1796053 0.1796053\n","  0.3592106 0.1796053 0.1796053 0.1796053 0.1796053 0.1796053 0.1796053\n","  0.3592106 0.1796053 0.3592106 0.3592106 0.1796053]]\n"]}],"source":["# Based on https://hackernoon.com/document-term-matrix-in-nlp-count-and-tf-idf-scores-explained\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","text = [\"You don’t want to waste your time. If you’re going to put aside the time and energy needed to learn new programming languages, you want to make sure, without a doubt, that the ones you choose are the most in-demand programming languages on the market. \"]\n","\n","vectorizer = TfidfVectorizer(stop_words='english', smooth_idf=True)\n","\n","input_matrix = vectorizer.fit_transform(text).todense()\n","print(input_matrix)"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.01540327 -0.01540327 -0.01540327 -0.01540327 -0.01540327 -0.01540327\n","  -0.01540327 -0.01540327 -0.06161308 -0.01540327 -0.01540327 -0.01540327\n","  -0.01540327 -0.01540327 -0.03080654 -0.01540327 -0.01540327 -0.01540327\n","  -0.03080654 -0.06161308 -0.01540327 -0.01540327 -0.01540327 -0.01540327\n","  -0.01540327 -0.04620981 -0.01540327 -0.01540327 -0.01540327 -0.01540327\n","  -0.01540327 -0.01540327 -0.01540327 -0.01540327 -0.01540327]]\n"]}],"source":["# Taken from ChatGTP\n","# Prompt : TF-IDF numpy - python\n","\n","import numpy as np\n","from collections import Counter\n","\n","# List of documents\n","documents = [\n","   \"You don’t want to waste your time. If you’re going to put aside the time and energy needed to learn new programming languages, you want to make sure, without a doubt, that the ones you choose are the most in-demand programming languages on the market. \"\n","]\n","\n","# Tokenization and preprocessing\n","def preprocess(text):\n","    text = text.lower()  # Convert to lowercase\n","    words = text.split()  # Split text into words\n","    return words\n","\n","tokenized_documents = [preprocess(doc) for doc in documents]\n","\n","# Calculate Term Frequency (TF)\n","def calculate_tf(document):\n","    word_counts = Counter(document)\n","    total_words = len(document)\n","    tf = {word: count / total_words for word, count in word_counts.items()}\n","    return tf\n","\n","# Calculate Document Frequency (DF) for each word\n","df = {}\n","for document in tokenized_documents:\n","    for word in set(document):\n","        df[word] = df.get(word, 0) + 1\n","\n","# Calculate Inverse Document Frequency (IDF)\n","num_documents = len(tokenized_documents)\n","idf = {word: np.log(num_documents / (df + 1)) for word, df in df.items()}\n","\n","# Create a matrix to store TF-IDF values\n","tfidf_matrix = np.zeros((len(documents), len(idf)))\n","\n","# Calculate TF-IDF values for each document\n","for i, document in enumerate(tokenized_documents):\n","    tf = calculate_tf(document)\n","    for j, word in enumerate(idf.keys()):\n","        tfidf_matrix[i, j] = tf.get(word, 0) * idf[word]\n","\n","# Print the TF-IDF matrix\n","print(tfidf_matrix)\n"]},{"cell_type":"markdown","metadata":{"id":"xMcsfndWJjm_"},"source":["### 5 - Comparación de documentos\n","Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"CZdiop6IJpZN"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.         0.13608276 0.33333333]\n"," [0.13608276 1.         0.13608276]\n"," [0.33333333 0.13608276 1.        ]]\n"]}],"source":["# Import libraries\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import pandas as pd\n","\n","document_a='The benefits of regular exercise for maintaining good health.'\n","document_b= 'Tips for a healthy diet and nutrition.'\n","document_c= 'The importance of proper sleep for overall well-being.'\n","\n","# Create the documen term matrix\n","corpus = [document_a, document_b, document_c]\n","count_vectorizer = CountVectorizer(stop_words='english')\n","count_vectorizer = CountVectorizer()\n","\n","# Convert the matrix to dataframe\n","sparse_matrix = count_vectorizer.fit_transform(corpus)\n","doc_term_matrix = sparse_matrix.todense()\n","df = pd.DataFrame(doc_term_matrix, \n","                  columns=count_vectorizer.get_feature_names_out(), \n","                  index=['document_a', 'docuement_b', 'document_'])\n","\n","df\n","\n","# Computing cosene similarity\n","print(cosine_similarity(df, df))\n"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.0, 0.6251670018737686, 0.7606283051410277]\n","[0.6251670018737686, 1.0, 0.6969603770188317]\n","[0.7606283051410277, 0.6969603770188317, 1.0]\n"]}],"source":["# Cosine similarity\n","# Taken from Chat GPT\n","import math\n","from collections import Counter\n","\n","# Function to calculate the dot product of two vectors\n","def dot_product(vector1, vector2):\n","    return sum(x * y for x, y in zip(vector1, vector2))\n","\n","# Function to calculate the magnitude (Euclidean norm) of a vector\n","def magnitude(vector):\n","    return math.sqrt(sum(x ** 2 for x in vector))\n","\n","# Function to calculate cosine similarity\n","def cosine_similarity(vector1, vector2):\n","    dot = dot_product(vector1, vector2)\n","    mag1 = magnitude(vector1)\n","    mag2 = magnitude(vector2)\n","    if mag1 == 0 or mag2 == 0:\n","        return 0  # Avoid division by zero\n","    return dot / (mag1 * mag2)\n","\n","# Corpus of documents (represented as lists of words)\n","\n","document_a='The benefits of regular exercise for maintaining good health.'\n","document_b= 'Tips for a healthy diet and nutrition.'\n","document_c= 'The importance of proper sleep for overall well-being.'\n","\n","# Create the documen term matrix\n","corpus = [document_a, document_b, document_c]\n","\n","# Calculate the TF (Term Frequency) vector for each document in the corpus\n","tf_vectors = []\n","for doc in corpus:\n","    word_count = Counter(doc)\n","    tf_vector = [word_count[word] / len(doc) for word in doc]\n","    tf_vectors.append(tf_vector)\n","\n","# Calculate the IDF (Inverse Document Frequency) vector for the corpus\n","idf_vector = []\n","for word in corpus[0]:\n","    doc_count = sum(1 for doc in corpus if word in doc)\n","    idf = math.log(len(corpus) / (1 + doc_count))\n","    idf_vector.append(idf)\n","\n","# Calculate the TF-IDF vector for each document in the corpus\n","tfidf_vectors = []\n","for tf_vector in tf_vectors:\n","    tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n","    tfidf_vectors.append(tfidf_vector)\n","\n","# Calculate the cosine similarity between all pairs of documents\n","cosine_similarities = []\n","for i in range(len(tfidf_vectors)):\n","    similarity_row = []\n","    for j in range(len(tfidf_vectors)):\n","        similarity = cosine_similarity(tfidf_vectors[i], tfidf_vectors[j])\n","        similarity_row.append(similarity)\n","    cosine_similarities.append(similarity_row)\n","\n","# Print the cosine similarity matrix\n","for row in cosine_similarities:\n","    print(row)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5fRYTpympAwJSVbric6dW","collapsed_sections":[],"name":"1a - word2vec.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"}},"nbformat":4,"nbformat_minor":0}
